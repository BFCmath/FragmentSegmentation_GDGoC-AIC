| **Model (Year)**                   | **License**                         | **Platform/Framework**                            | **Input**                     | **Performance (speed/accuracy)**                                                                                                                                                                 | **Pretrained & Integration**                                                                                                                                                            |
| ---------------------------------- | ----------------------------------- | ------------------------------------------------- | ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **MiDaS 3.x**<br>(2020–22)         | MIT (open)                          | Python/PyTorch (ONNX, OpenVINO)                   | Single image                  | Relative depth; high-quality output. Varieties: DPT/ViT models (e.g. 512×384 Beit, 1536×1024 DPT). Can run \~10–20 ms on GPU. Embedded versions (Swin-Tiny) trade accuracy for speed.            | Official PyTorch repo with many pretrained models; convertible to ONNX/OpenVINO.  TF‑Lite port for browser (TF.js demo); NCNN for Android. Widely used in PyTorch Hub.                  |
| **Depth Anything V1 (CVPR’24)**    | Apache‑2.0 (open)                   | Python/PyTorch (HuggingFace Transformers, CoreML) | Single image                  | State-of-the-art relative depth (beats MiDaS v3.1). Models (Small/Base/Large) run \~12–20 ms on V100 (3–12 ms on RTX4090 with TensorRT). Very fast on GPU (small model ≈3 ms).                   | Pretrained checkpoints on Hugging Face; official `transformers` pipeline. Apple CoreML versions available. Also supports ONNX export.                                                   |
| **Depth Anything V2 (NeurIPS’24)** | Apache‑2.0 (open)                   | Python/PyTorch (HuggingFace, CoreML)              | Single image                  | Improved detail and robustness over V1; “faster inference speed” with fewer params. Small/Base/Large/Giant models (24M–1.3B parameters). Small model suitable for near real-time on modern GPUs. | Pretrained models on Hugging Face; pipeline via `transformers`. CoreML support (iOS). Smaller “metric” versions also released for 4K+ depth. Integrates into Apple’s Vision frameworks. |
| **Metric3D (ICCV’23/TPAMI’24)**    | BSD‑2 (open)                        | Python/PyTorch, ONNX, Transformers                | Single image                  | SOTA **metric** depth and normals on KITTI/NYU. Top ranked in CVPR’23 depth challenge. Large ViT-based models. Inference not real-time on CPU; reasonable on GPU/ONNX.                           | Code and weights on GitHub (BSD-2). Supports ONNX (dynamic shapes). HuggingFace models available. Can be integrated via ONNX Runtime for cross-platform use.                            |
| **Depth Pro (ArXiv’24)**           | Apple license (free/non-commercial) | Python/PyTorch, CoreML                            | Single image                  | Zero-shot **metric** depth. Very sharp, high-res outputs; 2.25 MP depth in \~0.3 s on V100 (\~3.3 Hz). Focus on boundary detail. Likely slower on CPU.                                           | Reference implementation by Apple with pretrained weights. Can run via provided CLI or Python package. Apple TensorGraph/Core ML integration implied (Apple-developed).                 |
| **ZoeDepth (CVPR’23)**             | MIT (open)                          | Python/PyTorch, ONNX, HF                          | Single image                  | Zero-shot **metric** depth (scale-aware). Built on MiDaS backbone, better cross-dataset consistency. Good accuracy. Speed similar to MiDaS (dozens of ms on GPU).                                | Repo (isl-org/ZoeDepth) with checkpoints. Works as drop-in: `depth=zoodeth(model,image)`. ONNX support via PyTorch. Often used as backbone for PatchFusion.                             |
| **PatchFusion (CVPR’23)**          | MIT (open)                          | Python/PyTorch                                    | Single image                  | High-resolution metric depth by stitching many MiDaS/Zoe patches. Exceptional edge detail; **very slow**. Reportedly 16×–146× slower than ZoeDepth (e.g. minutes per image vs seconds).          | GitHub (zhyever/PatchFusion) with code and weights. Pre-trained on urban scenes. Mostly research/demo use; not real-time.                                                               |
| **Marigold (ECCV’24)**             | Apache‑2 (open)                     | Python/PyTorch (Diffusion)                        | Single image                  | Diffusion-based depth generation. Very fine detail; output is **relative (normalized)** depth. Extremely slow (multiple diffusion iterations).                                                   | GitHub (prs-eth/Marigold) with code and checkpoints. Not metric. Best for high-quality offline renderings.                                                                              |
| **AdaBins (ICCV’20)**              | GPL‑3.0 (open, copyleft)            | Python/PyTorch                                    | Single image                  | Transformer-based adaptive binning. Good accuracy on indoor/outdoor. Medium speed (e.g. \~0.1–0.2 s per 480p image on GPU). Trains output in continuous depth.                                   | Official PyTorch code with pretrained NYU/KITTI models. GPL-3 license. Can be converted to ONNX for deployment.                                                                         |
| **MonoDepth2 (ICCV’19)**           | CC BY‑NC (non-commercial)           | Python/PyTorch                                    | Single image (stereo-trained) | Self-supervised; moderate accuracy. Very fast (tens of FPS on GPU). Outputs scale-invariant disparity; needs scaling for metric.                                                                 | Niantic repo with pretrained models. Demo script for single image inference. License forbids commercial use.                                                                            |
